 ======================================================================
                       Training process started.                       
 ======================================================================
                                                                       
                          2018-02-08  14:53:41                         
                                                                       

 Copyright (C) 2015-2016 Nongnuch Artrith and Alexander Urban

 This program is distributed in the hope that it will be useful,
 but WITHOUT ANY WARRANTY; without even the implied warranty of
 MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 GNU General Public License in file `LICENSE' for more details.

 ----------------------------------------------------------------------
                              Parallel run                             
 ----------------------------------------------------------------------

 Number of processes : 1

 ----------------------------------------------------------------------
                                Networks                               
 ----------------------------------------------------------------------

 Restarting the Ti network from file : Ti.10t-10t.nn

 Number of layers :   4

 Number of nodes (without bias) 
 and activation type per layer :

       1 :    44
       2 :    10  hyperbolic tangent (tanh)
       3 :    10  hyperbolic tangent (tanh)
       4 :     1  linear function (linear)

 Required memory (words) :     193011 (   1507.90 KB)

 Total number of weights (incl. bias) :      571

 Restarting the O network from file : O.10t-10t.nn

 Number of layers :   4

 Number of nodes (without bias) 
 and activation type per layer :

       1 :    44
       2 :    10  hyperbolic tangent (tanh)
       3 :    10  hyperbolic tangent (tanh)
       4 :     1  linear function (linear)

 Required memory (words) :     193011 (   1507.90 KB)

 Total number of weights (incl. bias) :      571

 Timing info will be written to: train.time

 ----------------------------------------------------------------------
                            Storing networks                           
 ----------------------------------------------------------------------

 Saving the Ti network to file : Ti.10t-10t.nn
 Saving the O network to file : O.10t-10t.nn

 ----------------------------------------------------------------------
                           Training set info.                          
 ----------------------------------------------------------------------

 Training set file                   : TiO.train.scaled
 Number of structures in the data set: 7721

 Atomic species in training set      : 2
   Species : Ti O

 Average energy (eV/atom) : -0.828849
 Minimum energy (eV/atom) : -1.000000
 Maximum energy (eV/atom) : 0.999412

 The input and output values have been normalized to [-1.0, 1.0].
 Structures outside of this interval will not be used for training.
   Energy scaling factor: 0.186417
   Atomic energy shift  : 4.635678

 ----------------------------------------------------------------------
                            Training details                           
 ----------------------------------------------------------------------

 Training method         : Limited Memory BFGS

 Number of iterations    : 0

 Training structures     : 6949
 Testing  structures     : 772

 Testing set (restarted from previous run): 

       1        7       15       17       22       31       36       56 
      60       64       73       97      101      107      110      122 
     125      145      154      167      172      175      189      219 
     229      232      235      239      243      250      256      264 
     349      354      380      386      401      409      411      412 
     449      455      457      464      487      492      493      504 
     531      532      538      552      557      559      562      568 
     589      598      601      611      614      623      631      635 
     636      643      649      650      664      673      675      696 
     711      715      723      734      757      762      790      801 
     809      812      843      853      856      866      872      886 
     898      900      909      923      946      951      954      965 
     967      978      991      998     1017     1018     1023     1026 
    1033     1039     1057     1114     1121     1124     1128     1137 
    1147     1175     1179     1190     1192     1194     1197     1199 
    1204     1207     1208     1209     1222     1223     1232     1257 
    1269     1271     1283     1289     1309     1315     1320     1338 
    1342     1356     1373     1383     1421     1428     1482     1484 
    1490     1508     1512     1522     1528     1545     1555     1563 
    1574     1577     1578     1584     1624     1628     1629     1634 
    1644     1646     1647     1651     1655     1656     1663     1672 
    1689     1702     1730     1734     1769     1774     1781     1801 
    1835     1841     1866     1879     1883     1884     1886     1895 
    1900     1913     1942     1947     1980     1981     1989     2010 
    2018     2027     2046     2076     2086     2095     2102     2105 
    2106     2113     2118     2135     2138     2141     2143     2145 
    2155     2163     2167     2171     2202     2203     2204     2212 
    2215     2224     2236     2242     2256     2261     2262     2282 
    2283     2284     2298     2307     2313     2350     2375     2379 
    2400     2420     2424     2431     2441     2442     2451     2456 
    2467     2471     2479     2506     2507     2537     2538     2556 
    2562     2577     2602     2627     2637     2647     2657     2661 
    2662     2665     2677     2708     2710     2716     2723     2727 
    2728     2734     2736     2773     2788     2841     2856     2857 
    2862     2876     2883     2911     2931     2933     2935     2940 
    2950     2952     2970     3001     3011     3012     3023     3025 
    3028     3033     3038     3054     3074     3082     3094     3108 
    3123     3133     3143     3168     3173     3178     3190     3191 
    3203     3207     3224     3227     3248     3261     3270     3277 
    3278     3279     3280     3313     3361     3374     3378     3390 
    3398     3414     3415     3431     3462     3464     3470     3501 
    3511     3512     3513     3517     3522     3526     3543     3551 
    3552     3559     3560     3562     3578     3579     3581     3582 
    3591     3601     3615     3620     3621     3647     3650     3663 
    3667     3673     3680     3684     3708     3727     3798     3803 
    3806     3808     3809     3818     3825     3833     3847     3848 
    3849     3858     3866     3870     3879     3882     3888     3895 
    3900     3910     3922     3962     3983     3985     3991     4009 
    4039     4043     4048     4080     4084     4088     4090     4102 
    4112     4137     4165     4182     4193     4195     4197     4198 
    4203     4216     4227     4239     4261     4265     4287     4326 
    4337     4343     4350     4361     4365     4389     4391     4411 
    4437     4439     4455     4456     4491     4496     4515     4549 
    4554     4565     4571     4575     4579     4580     4586     4590 
    4611     4615     4616     4618     4646     4658     4690     4699 
    4703     4704     4713     4716     4727     4745     4771     4790 
    4792     4793     4813     4827     4830     4840     4841     4859 
    4860     4875     4885     4888     4915     4918     4927     4937 
    4946     4956     4975     4999     5000     5005     5031     5033 
    5050     5075     5084     5091     5136     5153     5154     5158 
    5192     5194     5198     5203     5222     5238     5249     5250 
    5261     5262     5267     5273     5275     5355     5356     5363 
    5380     5385     5391     5410     5416     5422     5443     5458 
    5459     5469     5482     5484     5485     5498     5532     5538 
    5546     5548     5552     5567     5576     5580     5581     5588 
    5589     5609     5611     5618     5621     5631     5639     5646 
    5653     5664     5677     5686     5710     5746     5749     5756 
    5812     5819     5820     5824     5830     5840     5844     5845 
    5855     5879     5883     5884     5896     5913     5919     5923 
    5926     5929     5949     5952     5980     5983     5998     6001 
    6007     6015     6022     6025     6026     6034     6043     6047 
    6056     6068     6088     6130     6160     6170     6175     6176 
    6179     6180     6181     6189     6200     6216     6231     6234 
    6238     6240     6244     6276     6281     6284     6285     6287 
    6299     6308     6309     6338     6345     6370     6380     6394 
    6401     6432     6434     6446     6463     6471     6475     6503 
    6504     6516     6523     6524     6548     6559     6569     6571 
    6593     6601     6604     6657     6671     6673     6689     6710 
    6729     6795     6796     6801     6815     6822     6836     6868 
    6878     6882     6886     6912     6914     6919     6923     6929 
    6949     6975     6987     6997     7003     7006     7015     7024 
    7029     7035     7036     7041     7047     7050     7052     7075 
    7088     7113     7125     7128     7144     7145     7149     7168 
    7171     7185     7189     7192     7221     7252     7258     7264 
    7269     7275     7288     7291     7300     7303     7313     7314 
    7319     7321     7325     7326     7334     7339     7350     7352 
    7358     7361     7364     7371     7379     7381     7393     7412 
    7420     7429     7444     7453     7461     7465     7471     7495 
    7499     7507     7550     7561     7573     7574     7592     7594 
    7597     7618     7629     7659     7666     7667     7668     7669 
    7670     7671     7672     7673     7674     7675     7676     7677 
    7678     7679     7680     7681     7682     7683     7684     7685 
    7686     7687     7688     7689     7690     7691     7692     7693 
    7694     7695     7696     7697     7698     7699     7700     7701 
    7702     7703     7704     7705     7706     7707     7708     7709 
    7710     7711     7712     7713     7714     7715     7716     7717 
    7718     7719     7720     7721 

 Attempting restart of optimization algorithm from file: train.restart
 Restarting L-BFGS.

 ----------------------------------------------------------------------
                            Training process                           
 ----------------------------------------------------------------------

 Weight optimization for 0 epochs using the Limited Memory BFGS method.

        |------------TRAIN-----------|  |------------TEST------------|
 epoch             MAE          <RMSE>             MAE          <RMSE>
     0    4.433366E-03    4.459498E-03    4.992455E-03    6.271533E-03 <

 Training finished.

 ----------------------------------------------------------------------
                         Storing final energies                        
 ----------------------------------------------------------------------

 Energies of training structures : energies.train.PROCESS
 Energies of testing structures  : energies.test.PROCESS
 (Manually concatenate the files from different processes.)

 Final MAE of training set  =      4.4 meV/atom
 Final MAE of testing set   =      5.0 meV/atom

 Final RMSE of training set =      4.5 meV/atom
 Final RMSE of testing set  =      6.3 meV/atom

 ----------------------------------------------------------------------
                            Storing networks                           
 ----------------------------------------------------------------------

 Saving the Ti network to file : Ti.10t-10t.nn
 Saving the O network to file : O.10t-10t.nn

                                                                       
                          2018-02-08  14:54:07                         
                                                                       
 ======================================================================
                     Neural Network training done.                     
 ======================================================================
